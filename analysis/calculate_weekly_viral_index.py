#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì£¼ê°„ ë‰´ìŠ¤ ë°”ì´ëŸ´ ì§€ìˆ˜ ê³„ì‚° ìŠ¤í¬ë¦½íŠ¸
1. DB(Supabase)ì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
2. ì£¼ê°„(Weekly) ë‹¨ìœ„ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ë¥¼ ì§‘ê³„
3. WoW, MA4 í¸ì°¨, Z-Scoreë¥¼ ê²°í•©í•˜ì—¬ ë°”ì´ëŸ´ ì§€ìˆ˜ ì‚°ì¶œ
4. ë¶„ì„ìš© CSV íŒŒì¼ë¡œ ì €ì¥
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

# Supabase ì„¤ì •
try:
    from supabase import create_client, Client
    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_KEY")
    if SUPABASE_URL and SUPABASE_KEY:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        SUPABASE_ENABLED = True
    else:
        SUPABASE_ENABLED = False
        print("[ì˜¤ë¥˜] Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
except ImportError:
    SUPABASE_ENABLED = False
    print("[ì˜¤ë¥˜] supabase ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def fetch_news_data_from_db():
    """Supabase news_2025_categorized í…Œì´ë¸”ì—ì„œ ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ (í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©)"""
    if not SUPABASE_ENABLED:
        return None
    
    print("ğŸ“‚ DBì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    table_name = "news_2025_categorized"
    all_data = []
    page_size = 1000
    offset = 0
    
    try:
        while True:
            res = supabase.table(table_name).select('news_date, category').range(offset, offset + page_size - 1).execute()
            data = res.data
            if not data:
                break
            all_data.extend(data)
            offset += page_size
            if len(data) < page_size:
                break
            print(f"  >> ë¡œë”© ì¤‘... ({len(all_data):,}ê°œ)", end="\r")
        
        df = pd.DataFrame(all_data)
        if df.empty:
            print("\n  >> ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        df = df.rename(columns={'news_date': 'date'})
        df['date'] = pd.to_datetime(df['date'])
        print(f"\n  >> ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ ê¸°ì‚¬")
        return df
    except Exception as e:
        print(f"\n  >> DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def calculate_weekly_viral_index(df):
    """ì£¼ê°„ ë°”ì´ëŸ´ ì§€ìˆ˜ ê³„ì‚° (WoW 40% + MA4í¸ì°¨ 40% + Z-Score 20%)"""
    print("\nğŸ“Š ì£¼ê°„ ë°”ì´ëŸ´ ì§€ìˆ˜ ì‚°ì¶œ ì¤‘...")
    
    # 1. ì£¼ê°„/ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜ ì§‘ê³„
    # 'W-MON'ì€ ì›”ìš”ì¼ ê¸°ì¤€ ì£¼ì°¨ë¥¼ ì˜ë¯¸ (êµë³´ë¬¸ê³  ì£¼ê°„ ê¸°ì¤€ê³¼ ë§ì¶¤)
    # ì¹´í…Œê³ ë¦¬ê°€ columnsë¡œ ì˜¤ë„ë¡ unstack
    weekly_counts = df.groupby([pd.Grouper(key='date', freq='W-MON'), 'category']).size().unstack(level=1).fillna(0)
    
    # 2. ì§€ìˆ˜ êµ¬ì„± ìš”ì†Œ ê³„ì‚°
    # 2-1. WoW (Week-over-Week) ì¦ê°€ìœ¨
    wow_growth = weekly_counts.pct_change() * 100
    
    # 2-2. 4ì£¼ ì´ë™í‰ê·  ëŒ€ë¹„ í¸ì°¨ (ìµœê·¼ íë¦„ ëŒ€ë¹„ ê¸‰ì¦ ì—¬ë¶€)
    ma4 = weekly_counts.rolling(window=4, min_periods=1).mean()
    ma_deviation = ((weekly_counts - ma4) / ma4) * 100
    
    # 2-3. Z-Score (ì¹´í…Œê³ ë¦¬ë³„ ì—­ì‚¬ì  ë³€ë™ì„± ëŒ€ë¹„ í˜„ì¬ ìˆ˜ì¤€)
    # 1ë…„ì¹˜ ë°ì´í„°ì´ë¯€ë¡œ ì „ì²´ í‰ê· /í‘œì¤€í¸ì°¨ ì‚¬ìš©
    z_scores = (weekly_counts - weekly_counts.mean()) / weekly_counts.std()
    
    # 3. ì¢…í•© ë°”ì´ëŸ´ ì§€ìˆ˜ (ê°€ì¤‘í•©)
    # ê²°ì¸¡ì¹˜(ì²« ì£¼ ë“±)ëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬
    viral_index = (
        wow_growth.fillna(0) * 0.4 +
        ma_deviation.fillna(0) * 0.4 +
        z_scores.fillna(0) * 10 * 0.2  # Z-score ìŠ¤ì¼€ì¼ ë³´ì • (10ë°°)
    )
    
    print(f"  >> ê³„ì‚° ì™„ë£Œ: {len(weekly_counts)}ê°œ ì£¼ì°¨ x {len(weekly_counts.columns)}ê°œ ì¹´í…Œê³ ë¦¬")
    return weekly_counts, viral_index

def main():
    print("=" * 60)
    print("[ì£¼ê°„ ë‰´ìŠ¤ ë°”ì´ëŸ´ ì§€ìˆ˜ ì‚°ì¶œ]")
    print("=" * 60)
    
    # 1. ë°ì´í„° ë¡œë“œ
    news_df = fetch_news_data_from_db()
    if news_df is None:
        return
    
    # 2. ì§€ìˆ˜ ê³„ì‚°
    counts_df, viral_df = calculate_weekly_viral_index(news_df)
    
    # 3. ë°ì´í„° êµ¬ì¡° ì •ë¦¬ (ë¶„ì„ìš© Long format)
    # (week, category, viral_index, article_count)
    viral_long = viral_df.stack().reset_index()
    viral_long.columns = ['week', 'category', 'viral_index']
    
    counts_long = counts_df.stack().reset_index()
    counts_long.columns = ['week', 'category', 'article_count']
    
    # ë‘ ë°ì´í„° ê²°í•©
    result_df = pd.merge(viral_long, counts_long, on=['week', 'category'])
    
    # 4. CSV ì €ì¥
    # ì €ì¥ ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
    output_dir = "/Users/minzzy/Desktop/statrack/book-review-analysis/analysis"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    output_path = os.path.join(output_dir, "weekly_news_viral_index.csv")
    result_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    print("\n" + "=" * 60)
    print(f"âœ… ì‘ì—… ì™„ë£Œ: {output_path}")
    print(f"   - ë°ì´í„° ê¸°ê°„: {result_df['week'].min().date()} ~ {result_df['week'].max().date()}")
    print(f"   - ì´ ë°ì´í„° í–‰: {len(result_df)}ê°œ")
    print("=" * 60)

if __name__ == "__main__":
    main()
