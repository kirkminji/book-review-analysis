{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì˜ˆì¸¡ ML ëª¨ë¸\n",
    "\n",
    "## ë¶„ì„ ê°œìš”\n",
    "- **ëª©í‘œ**: ë„ì„œì˜ ì£¼ê°„ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì… ì—¬ë¶€ ë° íŒë§¤ ì ìˆ˜ ì˜ˆì¸¡\n",
    "- **ì ‘ê·¼ë²•**: 2ë‹¨ê³„ ëª¨ë¸ (Two-Stage)\n",
    "  - 1ë‹¨ê³„ (ë¶„ë¥˜): ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì… ì—¬ë¶€ (0/1)\n",
    "  - 2ë‹¨ê³„ (íšŒê·€): ì§„ì… ì‹œ íŒë§¤ ì ìˆ˜ (1~20)\n",
    "- **ë°ì´í„° ë¶„í• **: ì‹œê°„ìˆœ Split (2025ë…„ 1~9ì›” train, 10~12ì›” test)\n",
    "- **Validation**: 2026ë…„ 1ì›” ë°ì´í„° (ì¶”í›„ ì ìš©)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_csv('./books_ml_dataset.csv')\n",
    "df['ymw'] = df['ymw'].astype(str)\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"   - í–‰ ìˆ˜: {len(df):,}ê°œ\")\n",
    "print(f\"   - ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ\")\n",
    "print(f\"\\nğŸ“‹ ì»¬ëŸ¼ ëª©ë¡:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. EDA (íƒìƒ‰ì  ë°ì´í„° ë¶„ì„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y ë³€ìˆ˜ ë¶„í¬ í™•ì¸\n",
    "print(\"ğŸ“Š Y ë³€ìˆ˜ (y_sales_score) ë¶„í¬\")\n",
    "print(\"=\"*50)\n",
    "print(df['y_sales_score'].describe())\n",
    "\n",
    "print(f\"\\në² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì… (score > 0): {(df['y_sales_score'] > 0).sum():,}ê°œ ({(df['y_sales_score'] > 0).mean()*100:.1f}%)\")\n",
    "print(f\"ë² ìŠ¤íŠ¸ì…€ëŸ¬ ë¯¸ì§„ì… (score = 0): {(df['y_sales_score'] == 0).sum():,}ê°œ ({(df['y_sales_score'] == 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y ë³€ìˆ˜ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# ì „ì²´ ë¶„í¬\n",
    "axes[0].hist(df['y_sales_score'], bins=21, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('y_sales_score ì „ì²´ ë¶„í¬')\n",
    "axes[0].set_xlabel('íŒë§¤ ì ìˆ˜')\n",
    "axes[0].set_ylabel('ë¹ˆë„')\n",
    "\n",
    "# ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì… ë°ì´í„°ë§Œ\n",
    "bestseller_scores = df[df['y_sales_score'] > 0]['y_sales_score']\n",
    "axes[1].hist(bestseller_scores, bins=20, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1].set_title('ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì… ë°ì´í„°ë§Œ (score > 0)')\n",
    "axes[1].set_xlabel('íŒë§¤ ì ìˆ˜')\n",
    "axes[1].set_ylabel('ë¹ˆë„')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì£¼ì°¨ë³„ ë¶„í¬ í™•ì¸\n",
    "df['year_month'] = df['ymw'].str[:6]  # YYYYMM ì¶”ì¶œ\n",
    "\n",
    "monthly_stats = df.groupby('year_month').agg({\n",
    "    'y_sales_score': ['count', 'mean', lambda x: (x > 0).sum()]\n",
    "}).round(2)\n",
    "monthly_stats.columns = ['ì´ í–‰ìˆ˜', 'í‰ê·  ì ìˆ˜', 'ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì… ìˆ˜']\n",
    "\n",
    "print(\"ğŸ“Š ì›”ë³„ ë°ì´í„° ë¶„í¬\")\n",
    "print(monthly_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”¼ì²˜ ìƒê´€ê´€ê³„ (Yì™€ì˜ ìƒê´€)\n",
    "feature_cols = [c for c in df.columns if c not in ['product_code', 'ymw', 'y_sales_score', 'year_month']]\n",
    "\n",
    "correlations = df[feature_cols + ['y_sales_score']].corr()['y_sales_score'].drop('y_sales_score').sort_values(ascending=False)\n",
    "\n",
    "print(\"ğŸ“Š Y ë³€ìˆ˜ì™€ì˜ ìƒê´€ê³„ìˆ˜ (ìƒìœ„/í•˜ìœ„ 10ê°œ)\")\n",
    "print(\"=\"*50)\n",
    "print(\"[ìƒìœ„ 10ê°œ]\")\n",
    "print(correlations.head(10))\n",
    "print(\"\\n[í•˜ìœ„ 10ê°œ]\")\n",
    "print(correlations.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒê´€ê³„ìˆ˜ ì‹œê°í™”\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlations.plot(kind='barh', color=['green' if x > 0 else 'red' for x in correlations])\n",
    "plt.title('í”¼ì²˜ë³„ Y ë³€ìˆ˜(y_sales_score)ì™€ì˜ ìƒê´€ê³„ìˆ˜')\n",
    "plt.xlabel('ìƒê´€ê³„ìˆ˜')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„í• \n\n### ê²°ì¸¡ì¹˜ í˜„í™© ë° ì›ì¸\n\n**ê²°ì¸¡ì¹˜ ë°œìƒ ì»¬ëŸ¼**: `category_1_predicted` ~ `category_10_predicted` (656ê°œ í–‰)\n\n**ê²°ì¸¡ì¹˜ ë°œìƒ ì£¼ì°¨**:\n- 2025035 (3ì›” 5ì£¼ì°¨)\n- 2025055 (5ì›” 5ì£¼ì°¨)\n- 2025085 (8ì›” 5ì£¼ì°¨)\n- 2025115 (11ì›” 5ì£¼ì°¨)\n\n**ì›ì¸**: ymw í˜•ì‹ ë¶ˆì¼ì¹˜\n```\në² ìŠ¤íŠ¸ì…€ëŸ¬ ë°ì´í„°:  2025035, 2025055, 2025085, 2025115 (3ì›”, 5ì›”, 8ì›”, 11ì›” 5ì£¼ì°¨)\në‰´ìŠ¤/Prophet ë°ì´í„°: 2025045, 2025075, 2025095, 2025125 (4ì›”, 7ì›”, 9ì›”, 12ì›” 5ì£¼ì°¨)\n```\nâ†’ ë‘ ë°ì´í„° ì†ŒìŠ¤ì˜ **ì›”ë³„ ì£¼ì°¨ ê³„ì‚° ë°©ì‹ì´ ë‹¤ë¦„**\nâ†’ Prophet ì˜ˆì¸¡ê°’ì´ í•´ë‹¹ ì£¼ì°¨ì— ì¡´ì¬í•˜ì§€ ì•Šì•„ ê²°ì¸¡ì¹˜ ë°œìƒ\n\n**ì²˜ë¦¬ ë°©ë²•**: ê²°ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ëŒ€ì²´ (í•´ë‹¹ ì£¼ì°¨ì—ëŠ” Prophet ê¸°ë°˜ ì˜ˆì¸¡ í”¼ì²˜ ë¯¸ë°˜ì˜)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”¼ì²˜ ë° íƒ€ê²Ÿ ë¶„ë¦¬\n",
    "feature_cols = [c for c in df.columns if c not in ['product_code', 'ymw', 'y_sales_score', 'year_month']]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y_score = df['y_sales_score'].copy()  # íšŒê·€ìš©\n",
    "y_binary = (df['y_sales_score'] > 0).astype(int)  # ë¶„ë¥˜ìš© (ì§„ì… ì—¬ë¶€)\n",
    "\n",
    "print(f\"âœ… í”¼ì²˜/íƒ€ê²Ÿ ë¶„ë¦¬ ì™„ë£Œ\")\n",
    "print(f\"   - í”¼ì²˜ ìˆ˜: {len(feature_cols)}ê°œ\")\n",
    "print(f\"   - ë¶„ë¥˜ íƒ€ê²Ÿ (y_binary): 0={y_binary.value_counts()[0]:,}, 1={y_binary.value_counts()[1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°„ìˆœ ë¶„í•  (2025ë…„ 1~9ì›”: train, 10~12ì›”: test)\n",
    "# ymw í˜•ì‹: YYYYMMW (ì˜ˆ: 2025091 = 2025ë…„ 9ì›” 1ì£¼ì°¨)\n",
    "\n",
    "train_mask = df['ymw'].str[:6].astype(int) <= 202509  # 9ì›”ê¹Œì§€\n",
    "test_mask = df['ymw'].str[:6].astype(int) >= 202510   # 10ì›”ë¶€í„°\n",
    "\n",
    "X_train, X_test = X[train_mask], X[test_mask]\n",
    "y_train_binary, y_test_binary = y_binary[train_mask], y_binary[test_mask]\n",
    "y_train_score, y_test_score = y_score[train_mask], y_score[test_mask]\n",
    "\n",
    "print(f\"âœ… ì‹œê°„ìˆœ ë¶„í•  ì™„ë£Œ\")\n",
    "print(f\"   - Train (2025ë…„ 1~9ì›”): {len(X_train):,}ê°œ\")\n",
    "print(f\"   - Test (2025ë…„ 10~12ì›”): {len(X_test):,}ê°œ\")\n",
    "print(f\"\\n   - Train ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì…ë¥ : {y_train_binary.mean()*100:.1f}%\")\n",
    "print(f\"   - Test ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì…ë¥ : {y_test_binary.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìŠ¤ì¼€ì¼ë§\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# DataFrameìœ¼ë¡œ ë³€í™˜ (í”¼ì²˜ ì´ë¦„ ìœ ì§€)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
    "\n",
    "print(\"âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. 1ë‹¨ê³„: ë¶„ë¥˜ ëª¨ë¸ (ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì… ì˜ˆì¸¡)\n",
    "\n",
    "ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•´ `class_weight='balanced'` ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„ë¥˜ ëª¨ë¸ ì •ì˜\n",
    "clf_models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(scale_pos_weight=len(y_train_binary[y_train_binary==0])/len(y_train_binary[y_train_binary==1]), \n",
    "                            random_state=42, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(class_weight='balanced', random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“Œ ë¶„ë¥˜ ëª¨ë¸ {len(clf_models)}ê°œ ì •ì˜ ì™„ë£Œ\")\n",
    "print(f\"   - ë¶ˆê· í˜• ì²˜ë¦¬: class_weight='balanced' ë˜ëŠ” scale_pos_weight ì ìš©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
    "clf_results = []\n",
    "\n",
    "for name, model in clf_models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"[{name}] í•™ìŠµ ì¤‘...\")\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    model.fit(X_train_scaled, y_train_binary)\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # í‰ê°€\n",
    "    acc = accuracy_score(y_test_binary, y_pred)\n",
    "    prec = precision_score(y_test_binary, y_pred)\n",
    "    rec = recall_score(y_test_binary, y_pred)\n",
    "    f1 = f1_score(y_test_binary, y_pred)\n",
    "    auc = roc_auc_score(y_test_binary, y_pred_proba)\n",
    "    \n",
    "    clf_results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': round(acc, 4),\n",
    "        'Precision': round(prec, 4),\n",
    "        'Recall': round(rec, 4),\n",
    "        'F1-Score': round(f1, 4),\n",
    "        'AUC-ROC': round(auc, 4)\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy: {acc:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall: {rec:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "clf_results_df = pd.DataFrame(clf_results).sort_values('F1-Score', ascending=False)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (F1-Score ê¸°ì¤€ ì •ë ¬)\")\n",
    "print(\"=\"*60)\n",
    "print(clf_results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ Confusion Matrix\n",
    "best_clf_name = clf_results_df.iloc[0]['Model']\n",
    "best_clf_model = clf_models[best_clf_name]\n",
    "y_pred_best = best_clf_model.predict(X_test_scaled)\n",
    "\n",
    "cm = confusion_matrix(y_test_binary, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['ë¯¸ì§„ì…(0)', 'ì§„ì…(1)'], \n",
    "            yticklabels=['ë¯¸ì§„ì…(0)', 'ì§„ì…(1)'])\n",
    "plt.title(f'Confusion Matrix - {best_clf_name}')\n",
    "plt.xlabel('ì˜ˆì¸¡')\n",
    "plt.ylabel('ì‹¤ì œ')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“‹ Classification Report - {best_clf_name}\")\n",
    "print(classification_report(y_test_binary, y_pred_best, target_names=['ë¯¸ì§„ì…', 'ì§„ì…']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶„ë¥˜ ëª¨ë¸ Feature Importance\n",
    "if hasattr(best_clf_model, 'feature_importances_'):\n",
    "    importance = best_clf_model.feature_importances_\n",
    "elif hasattr(best_clf_model, 'coef_'):\n",
    "    importance = np.abs(best_clf_model.coef_[0])\n",
    "else:\n",
    "    importance = None\n",
    "\n",
    "if importance is not None:\n",
    "    feat_imp_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(data=feat_imp_df.head(20), x='importance', y='feature', palette='viridis')\n",
    "    plt.title(f'Feature Importance (ë¶„ë¥˜) - {best_clf_name}')\n",
    "    plt.xlabel('ì¤‘ìš”ë„')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ“Š ìƒìœ„ 10ê°œ í”¼ì²˜\")\n",
    "    print(feat_imp_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. 2ë‹¨ê³„: íšŒê·€ ëª¨ë¸ (íŒë§¤ ì ìˆ˜ ì˜ˆì¸¡)\n",
    "\n",
    "ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì… ë°ì´í„°ë§Œ ì‚¬ìš© (y_sales_score > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì… ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "train_bs_mask = (y_train_score > 0)\n",
    "test_bs_mask = (y_test_score > 0)\n",
    "\n",
    "X_train_bs = X_train_scaled[train_bs_mask]\n",
    "X_test_bs = X_test_scaled[test_bs_mask]\n",
    "y_train_bs = y_train_score[train_bs_mask]\n",
    "y_test_bs = y_test_score[test_bs_mask]\n",
    "\n",
    "print(f\"âœ… ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì… ë°ì´í„° í•„í„°ë§ ì™„ë£Œ\")\n",
    "print(f\"   - Train: {len(X_train_bs):,}ê°œ\")\n",
    "print(f\"   - Test: {len(X_test_bs):,}ê°œ\")\n",
    "print(f\"\\n   - Train í‰ê·  ì ìˆ˜: {y_train_bs.mean():.2f}\")\n",
    "print(f\"   - Test í‰ê·  ì ìˆ˜: {y_test_bs.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íšŒê·€ ëª¨ë¸ ì •ì˜\n",
    "reg_models = {\n",
    "    'Ridge': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso': Lasso(alpha=0.1, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'LightGBM': LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“Œ íšŒê·€ ëª¨ë¸ {len(reg_models)}ê°œ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íšŒê·€ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
    "reg_results = []\n",
    "\n",
    "for name, model in reg_models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"[{name}] í•™ìŠµ ì¤‘...\")\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    model.fit(X_train_bs, y_train_bs)\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    y_pred = model.predict(X_test_bs)\n",
    "    \n",
    "    # í‰ê°€\n",
    "    mse = mean_squared_error(y_test_bs, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test_bs, y_pred)\n",
    "    r2 = r2_score(y_test_bs, y_pred)\n",
    "    \n",
    "    reg_results.append({\n",
    "        'Model': name,\n",
    "        'RMSE': round(rmse, 4),\n",
    "        'MAE': round(mae, 4),\n",
    "        'RÂ²': round(r2, 4)\n",
    "    })\n",
    "    \n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  RÂ²: {r2:.4f}\")\n",
    "\n",
    "reg_results_df = pd.DataFrame(reg_results).sort_values('RÂ²', ascending=False)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (RÂ² ê¸°ì¤€ ì •ë ¬)\")\n",
    "print(\"=\"*60)\n",
    "print(reg_results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœê³  ì„±ëŠ¥ íšŒê·€ ëª¨ë¸ì˜ ì˜ˆì¸¡ vs ì‹¤ì œ ì‹œê°í™”\n",
    "best_reg_name = reg_results_df.iloc[0]['Model']\n",
    "best_reg_model = reg_models[best_reg_name]\n",
    "y_pred_best_reg = best_reg_model.predict(X_test_bs)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ì‚°ì ë„\n",
    "axes[0].scatter(y_test_bs, y_pred_best_reg, alpha=0.5)\n",
    "axes[0].plot([0, 21], [0, 21], 'r--', label='Perfect Fit')\n",
    "axes[0].set_xlabel('ì‹¤ì œ ì ìˆ˜')\n",
    "axes[0].set_ylabel('ì˜ˆì¸¡ ì ìˆ˜')\n",
    "axes[0].set_title(f'ì˜ˆì¸¡ vs ì‹¤ì œ - {best_reg_name}')\n",
    "axes[0].legend()\n",
    "\n",
    "# ì”ì°¨ ë¶„í¬\n",
    "residuals = y_test_bs - y_pred_best_reg\n",
    "axes[1].hist(residuals, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--')\n",
    "axes[1].set_xlabel('ì”ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)')\n",
    "axes[1].set_ylabel('ë¹ˆë„')\n",
    "axes[1].set_title(f'ì”ì°¨ ë¶„í¬ - {best_reg_name}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íšŒê·€ ëª¨ë¸ Feature Importance\n",
    "if hasattr(best_reg_model, 'feature_importances_'):\n",
    "    importance_reg = best_reg_model.feature_importances_\n",
    "elif hasattr(best_reg_model, 'coef_'):\n",
    "    importance_reg = np.abs(best_reg_model.coef_)\n",
    "else:\n",
    "    importance_reg = None\n",
    "\n",
    "if importance_reg is not None:\n",
    "    feat_imp_reg_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': importance_reg\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(data=feat_imp_reg_df.head(20), x='importance', y='feature', palette='magma')\n",
    "    plt.title(f'Feature Importance (íšŒê·€) - {best_reg_name}')\n",
    "    plt.xlabel('ì¤‘ìš”ë„')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ“Š ìƒìœ„ 10ê°œ í”¼ì²˜ (íšŒê·€)\")\n",
    "    print(feat_imp_reg_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "\n",
    "# ë¶„ë¥˜ ëª¨ë¸ ê¸°ë°˜ Feature Selection\n",
    "print(\"ğŸ“Œ Feature Selection (ë¶„ë¥˜ ëª¨ë¸ ê¸°ì¤€)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# LightGBM ê¸°ë°˜ SelectFromModel\n",
    "selector_clf = SelectFromModel(LGBMClassifier(class_weight='balanced', random_state=42, verbose=-1), \n",
    "                               threshold='median')\n",
    "selector_clf.fit(X_train_scaled, y_train_binary)\n",
    "\n",
    "selected_features_clf = np.array(feature_cols)[selector_clf.get_support()]\n",
    "print(f\"\\nì„ íƒëœ í”¼ì²˜ ìˆ˜: {len(selected_features_clf)}ê°œ\")\n",
    "print(f\"ì„ íƒëœ í”¼ì²˜: {list(selected_features_clf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ íƒëœ í”¼ì²˜ë¡œ ë¶„ë¥˜ ëª¨ë¸ ì¬í•™ìŠµ\n",
    "X_train_selected = X_train_scaled[selected_features_clf]\n",
    "X_test_selected = X_test_scaled[selected_features_clf]\n",
    "\n",
    "print(\"ğŸ“Œ ì„ íƒëœ í”¼ì²˜ë¡œ ë¶„ë¥˜ ëª¨ë¸ ì¬í•™ìŠµ\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "clf_selected_results = []\n",
    "\n",
    "for name, model in clf_models.items():\n",
    "    # ìƒˆ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "    if name == 'Logistic Regression':\n",
    "        m = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "    elif name == 'Random Forest':\n",
    "        m = RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42)\n",
    "    elif name == 'XGBoost':\n",
    "        m = XGBClassifier(scale_pos_weight=len(y_train_binary[y_train_binary==0])/len(y_train_binary[y_train_binary==1]), \n",
    "                        random_state=42, eval_metric='logloss')\n",
    "    else:\n",
    "        m = LGBMClassifier(class_weight='balanced', random_state=42, verbose=-1)\n",
    "    \n",
    "    m.fit(X_train_selected, y_train_binary)\n",
    "    y_pred = m.predict(X_test_selected)\n",
    "    y_pred_proba = m.predict_proba(X_test_selected)[:, 1]\n",
    "    \n",
    "    clf_selected_results.append({\n",
    "        'Model': name,\n",
    "        'F1-Score': round(f1_score(y_test_binary, y_pred), 4),\n",
    "        'AUC-ROC': round(roc_auc_score(y_test_binary, y_pred_proba), 4)\n",
    "    })\n",
    "\n",
    "clf_selected_df = pd.DataFrame(clf_selected_results).sort_values('F1-Score', ascending=False)\n",
    "print(\"\\nğŸ“Š Feature Selection í›„ ë¶„ë¥˜ ì„±ëŠ¥\")\n",
    "print(clf_selected_df.to_string(index=False))\n",
    "\n",
    "# ë¹„êµ\n",
    "print(\"\\nğŸ“Š Feature Selection ì „í›„ ë¹„êµ (F1-Score)\")\n",
    "comparison = pd.merge(\n",
    "    clf_results_df[['Model', 'F1-Score']].rename(columns={'F1-Score': 'Before'}),\n",
    "    clf_selected_df[['Model', 'F1-Score']].rename(columns={'F1-Score': 'After'}),\n",
    "    on='Model'\n",
    ")\n",
    "comparison['Diff'] = comparison['After'] - comparison['Before']\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. ê²°ê³¼ ìš”ì•½ ë° í•´ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[1ë‹¨ê³„: ë¶„ë¥˜ ëª¨ë¸ - ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì… ì˜ˆì¸¡]\")\n",
    "print(\"-\"*50)\n",
    "print(f\"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {clf_results_df.iloc[0]['Model']}\")\n",
    "print(f\"  - F1-Score: {clf_results_df.iloc[0]['F1-Score']}\")\n",
    "print(f\"  - AUC-ROC: {clf_results_df.iloc[0]['AUC-ROC']}\")\n",
    "print(f\"  - Precision: {clf_results_df.iloc[0]['Precision']}\")\n",
    "print(f\"  - Recall: {clf_results_df.iloc[0]['Recall']}\")\n",
    "\n",
    "print(\"\\n[2ë‹¨ê³„: íšŒê·€ ëª¨ë¸ - íŒë§¤ ì ìˆ˜ ì˜ˆì¸¡]\")\n",
    "print(\"-\"*50)\n",
    "print(f\"ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {reg_results_df.iloc[0]['Model']}\")\n",
    "print(f\"  - RÂ²: {reg_results_df.iloc[0]['RÂ²']}\")\n",
    "print(f\"  - RMSE: {reg_results_df.iloc[0]['RMSE']}\")\n",
    "print(f\"  - MAE: {reg_results_df.iloc[0]['MAE']}\")\n",
    "\n",
    "print(\"\\n[Feature Selection ê²°ê³¼]\")\n",
    "print(\"-\"*50)\n",
    "print(f\"ì „ì²´ í”¼ì²˜ ìˆ˜: {len(feature_cols)}ê°œ\")\n",
    "print(f\"ì„ íƒëœ í”¼ì²˜ ìˆ˜: {len(selected_features_clf)}ê°œ\")\n",
    "print(f\"ì„ íƒëœ í”¼ì²˜: {list(selected_features_clf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ë¶„ë¥˜ ëª¨ë¸ ë¹„êµ\n",
    "ax1 = axes[0]\n",
    "x = range(len(clf_results_df))\n",
    "width = 0.35\n",
    "ax1.bar([i - width/2 for i in x], clf_results_df['F1-Score'], width, label='F1-Score', color='steelblue')\n",
    "ax1.bar([i + width/2 for i in x], clf_results_df['AUC-ROC'], width, label='AUC-ROC', color='orange')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(clf_results_df['Model'], rotation=45, ha='right')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ')\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# íšŒê·€ ëª¨ë¸ ë¹„êµ\n",
    "ax2 = axes[1]\n",
    "colors = ['green' if r > 0 else 'red' for r in reg_results_df['RÂ²']]\n",
    "ax2.barh(reg_results_df['Model'], reg_results_df['RÂ²'], color=colors)\n",
    "ax2.set_xlabel('RÂ² Score')\n",
    "ax2.set_title('íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (RÂ²)')\n",
    "ax2.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "joblib.dump(best_clf_model, './best_classifier.pkl')\n",
    "joblib.dump(best_reg_model, './best_regressor.pkl')\n",
    "joblib.dump(scaler, './scaler.pkl')\n",
    "\n",
    "# ê²°ê³¼ CSV ì €ì¥\n",
    "clf_results_df.to_csv('./classification_results.csv', index=False)\n",
    "reg_results_df.to_csv('./regression_results.csv', index=False)\n",
    "\n",
    "print(\"âœ… ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥ ì™„ë£Œ\")\n",
    "print(\"   - best_classifier.pkl\")\n",
    "print(\"   - best_regressor.pkl\")\n",
    "print(\"   - scaler.pkl\")\n",
    "print(\"   - classification_results.csv\")\n",
    "print(\"   - regression_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. ì¸ì‚¬ì´íŠ¸ ë° í•´ì„\n",
    "\n",
    "### ë¶„ë¥˜ ëª¨ë¸ í•´ì„\n",
    "- **ëª©í‘œ**: íŠ¹ì • ë„ì„œê°€ íŠ¹ì • ì£¼ì°¨ì— ë² ìŠ¤íŠ¸ì…€ëŸ¬ Top 20ì— ì§„ì…í• ì§€ ì˜ˆì¸¡\n",
    "- **ë¶ˆê· í˜• ì²˜ë¦¬**: class_weight='balanced' ì ìš©ìœ¼ë¡œ ì†Œìˆ˜ í´ë˜ìŠ¤(ì§„ì…) í•™ìŠµ ê°•í™”\n",
    "- **ì£¼ìš” ì§€í‘œ**: F1-Score, AUC-ROC (ë¶ˆê· í˜• ë°ì´í„°ì—ì„œ Accuracyë³´ë‹¤ ì‹ ë¢°ì„± ë†’ìŒ)\n",
    "\n",
    "### íšŒê·€ ëª¨ë¸ í•´ì„\n",
    "- **ëª©í‘œ**: ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì… ì‹œ ì˜ˆìƒ ìˆœìœ„ ì ìˆ˜ (1~20ì ) ì˜ˆì¸¡\n",
    "- **ë°ì´í„°**: ì‹¤ì œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì… ë°ì´í„°ë§Œ ì‚¬ìš©\n",
    "- **ì£¼ìš” ì§€í‘œ**: RÂ², RMSE, MAE\n",
    "\n",
    "### Feature Importance ì¸ì‚¬ì´íŠ¸\n",
    "- ì¤‘ìš” í”¼ì²˜ ë¶„ì„ì„ í†µí•´ ì–´ë–¤ ìš”ì¸ì´ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì§„ì…/ìˆœìœ„ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ íŒŒì•…\n",
    "- ë°”ì´ëŸ´ ì§€ìˆ˜, ì¹´í…Œê³ ë¦¬ ê´€ë ¨ì„±, ê±°ì‹œê²½ì œ ì§€í‘œì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ ë¹„êµ\n",
    "\n",
    "### ë¹„ì¦ˆë‹ˆìŠ¤ í™œìš©\n",
    "1. **ì¶œíŒì‚¬**: ì‹ ê°„ ì¶œì‹œ ì‹œì  ìµœì í™”, ë§ˆì¼€íŒ… ëŒ€ìƒ ë„ì„œ ì„ ì •\n",
    "2. **ì„œì **: ì¬ê³  ê´€ë¦¬, í”„ë¡œëª¨ì…˜ ê¸°íš\n",
    "3. **ì €ì/ì—ì´ì „ì‹œ**: ì¶œê°„ íƒ€ì´ë° ì „ëµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. ë‹¤ìŒ ë‹¨ê³„ (TODO)\n",
    "\n",
    "1. **2026ë…„ 1ì›” Validation ë°ì´í„° ì ìš©**\n",
    "   - ë³„ë„ ìˆ˜ì§‘ í›„ ëª¨ë¸ ê²€ì¦\n",
    "   - ì‹¤ì œ ìš´ì˜ í™˜ê²½ ì„±ëŠ¥ í™•ì¸\n",
    "\n",
    "2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**\n",
    "   - GridSearchCV ë˜ëŠ” Optuna í™œìš©\n",
    "   - ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰\n",
    "\n",
    "3. **ì•™ìƒë¸” ëª¨ë¸ êµ¬ì„±**\n",
    "   - ë¶„ë¥˜ + íšŒê·€ íŒŒì´í”„ë¼ì¸\n",
    "   - Stacking, Voting ë“±\n",
    "\n",
    "4. **Prophet í”¼ì²˜ ì¶”ê°€**\n",
    "   - Walk-Forward Prophet ì˜ˆì¸¡ê°’ í”¼ì²˜ë¡œ ì¶”ê°€\n",
    "   - ì‹œì°¨ íš¨ê³¼ ë°˜ì˜"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}