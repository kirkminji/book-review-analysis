#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì£¼ê°„ ë‰´ìŠ¤ ë°”ì´ëŸ´ ì§€ìˆ˜ ê³„ì‚° ìŠ¤í¬ë¦½íŠ¸ (DB ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì£¼ì°¨ ê¸°ì¤€)
1. DB(Supabase)ì—ì„œ ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
2. DBì˜ 'weekly_bestsellers' í…Œì´ë¸”ì—ì„œ ì£¼ì°¨ ì •ì˜ë¥¼ ê°€ì ¸ì˜´ (ymw, bestseller_week)
3. ê° ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì£¼ì°¨ êµ¬ê°„ì— ë§ì¶° ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ë¥¼ ì§‘ê³„
4. WoW, MA4 í¸ì°¨, Z-Scoreë¥¼ ê²°í•©í•˜ì—¬ ë°”ì´ëŸ´ ì§€ìˆ˜ ë° Smoothing ì§€ìˆ˜ ì‚°ì¶œ
5. ë¶„ì„ìš© CSV íŒŒì¼ë¡œ ì €ì¥ (weekly_news_viral_index_revised.csv)
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

try:
    from supabase import create_client, Client
    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_KEY")
    if SUPABASE_URL and SUPABASE_KEY:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        SUPABASE_ENABLED = True
    else:
        SUPABASE_ENABLED = False
        print("[ì˜¤ë¥˜] Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
except ImportError:
    SUPABASE_ENABLED = False
    print("[ì˜¤ë¥˜] supabase ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def fetch_news_data_from_db():
    if not SUPABASE_ENABLED:
        return None
    
    print("ğŸ“‚ DBì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    table_name = "news_2025_categorized"
    all_data = []
    page_size = 1000
    offset = 0
    
    try:
        while True:
            res = supabase.table(table_name).select('news_date, category').range(offset, offset + page_size - 1).execute()
            data = res.data
            if not data:
                break
            all_data.extend(data)
            offset += page_size
            if len(data) < page_size:
                break
            print(f"  >> ë¡œë”© ì¤‘... ({len(all_data):,}ê°œ)", end="\r")
        
        df = pd.DataFrame(all_data)
        if df.empty:
            print("\n  >> ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        df = df.rename(columns={'news_date': 'date'})
        df['date'] = pd.to_datetime(df['date'])
        print(f"\n  >> ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ ê¸°ì‚¬")
        return df
    except Exception as e:
        print(f"\n  >> DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def calculate_viral_indices(weekly_counts_df):
    print("\nğŸ“Š ì£¼ê°„ ë°”ì´ëŸ´ ì§€ìˆ˜ ë° Smoothing ì§€ìˆ˜ ì‚°ì¶œ ì¤‘...")
    
    # ì§€ìˆ˜ êµ¬ì„± ìš”ì†Œ ê³„ì‚° (ì•ˆì •ì„± ê°•í™”)
    prev_counts = weekly_counts_df.shift(1)
    wow_growth = ((weekly_counts_df - prev_counts) / (prev_counts + 1)) * 100
    
    ma4 = weekly_counts_df.rolling(window=4, min_periods=1).mean()
    ma_deviation = ((weekly_counts_df - ma4) / (ma4 + 1)) * 100
    
    z_scores = (weekly_counts_df - weekly_counts_df.mean()) / (weekly_counts_df.std() + 1e-9)
    
    viral_index = (
        wow_growth.clip(upper=300).fillna(0) * 0.4 +
        ma_deviation.clip(upper=300).fillna(0) * 0.4 +
        z_scores.clip(lower=-3, upper=3).fillna(0) * 10 * 0.2
    )
    
    viral_index_smoothed = viral_index.rolling(window=2, min_periods=1).mean()
    
    print(f"  >> ê³„ì‚° ì™„ë£Œ: {len(weekly_counts_df)}ê°œ ì£¼ì°¨ x {len(weekly_counts_df.columns)}ê°œ ì¹´í…Œê³ ë¦¬")
    return viral_index, viral_index_smoothed

def main():
    print("=" * 60)
    print("[ì£¼ê°„ ë‰´ìŠ¤ ë°”ì´ëŸ´ ì§€ìˆ˜ ì‚°ì¶œ (DB ì£¼ì°¨ ê¸°ì¤€)]")
    print("=" * 60)
    
    news_df = fetch_news_data_from_db()
    if news_df is None:
        return

    print("ğŸ“‚ DBì—ì„œ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì£¼ì°¨ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    try:
        res_bs_weeks = supabase.table('weekly_bestsellers').select('ymw, bestseller_week').order('ymw').execute()
        df_bs_weeks_raw = pd.DataFrame(res_bs_weeks.data).drop_duplicates()
        
        def parse_week_string(week_str):
            parts = week_str.split(' ~ ')
            return pd.to_datetime(parts[0]), pd.to_datetime(parts[1])
        
        df_bs_weeks_raw[['start_date', 'end_date']] = df_bs_weeks_raw['bestseller_week'].apply(lambda x: pd.Series(parse_week_string(x)))
        df_bs_weeks = df_bs_weeks_raw.sort_values('start_date').reset_index(drop=True)

        if df_bs_weeks.empty:
            print("  >> ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì£¼ì°¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"  >> ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì£¼ì°¨ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {len(df_bs_weeks)}ê°œ ì£¼ì°¨")
    except Exception as e:
        print(f"  >> ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì£¼ì°¨ ì •ë³´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    all_weekly_news_counts = []
    all_category_names = news_df['category'].unique()
    
    print("\nğŸ“Š ê° DB ì£¼ì°¨ë³„ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ ì§‘ê³„ ì¤‘...")
    for idx, row in df_bs_weeks.iterrows():
        ymw_bs = row['ymw']
        bestseller_week_str = row['bestseller_week']
        start_date_bs = row['start_date']
        end_date_bs = row['end_date']
        
        # í•´ë‹¹ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì£¼ì°¨ êµ¬ê°„ì— í•´ë‹¹í•˜ëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§
        week_news_df = news_df[(news_df['date'] >= start_date_bs) & (news_df['date'] <= end_date_bs)]
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜ ì§‘ê³„ (ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬í•¨)
        counts_series = week_news_df['category'].value_counts().reindex(all_category_names, fill_value=0)
        
        for category, count in counts_series.items():
            all_weekly_news_counts.append({
                'ymw': ymw_bs,
                'bestseller_week': bestseller_week_str,
                'category': category,
                'article_count': count,
                'start_date': start_date_bs,
                'end_date': end_date_bs
            })
    
    df_news_counts_long = pd.DataFrame(all_weekly_news_counts)
    
    if df_news_counts_long.empty:
        print("  >> ì§‘ê³„ëœ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë°”ì´ëŸ´ ì§€ìˆ˜ ê³„ì‚°ì„ ìœ„í•´ wide formatìœ¼ë¡œ ë³€í™˜
    df_pivoted_counts = df_news_counts_long.pivot_table(index='ymw', columns='category', values='article_count', fill_value=0)
    df_pivoted_counts.index = df_pivoted_counts.index.astype(str) # ensure ymw is string

    # ë°”ì´ëŸ´ ì§€ìˆ˜ ê³„ì‚° í•¨ìˆ˜ í˜¸ì¶œ
    viral_index_df, viral_index_smoothed_df = calculate_viral_indices(df_pivoted_counts)

    # ê²°ê³¼ë¥¼ ë‹¤ì‹œ long formatìœ¼ë¡œ ë³‘í•©
    viral_long = viral_index_df.stack().reset_index()
    viral_long.columns = ['ymw', 'category', 'viral_index']
    
    viral_smoothed_long = viral_index_smoothed_df.stack().reset_index()
    viral_smoothed_long.columns = ['ymw', 'category', 'viral_index_smoothed']
    
    result_df = pd.merge(df_news_counts_long, viral_long, on=['ymw', 'category'], how='left')
    result_df = pd.merge(result_df, viral_smoothed_long, on=['ymw', 'category'], how='left')

    # ìµœì¢… ì»¬ëŸ¼ ìˆœì„œ ì¡°ì •
    result_df = result_df[['ymw', 'bestseller_week', 'category', 'viral_index', 'viral_index_smoothed', 'article_count', 'start_date', 'end_date']]
    
    output_dir = "/Users/minzzy/Desktop/statrack/book-review-analysis/analysis"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    output_path = os.path.join(output_dir, "weekly_news_viral_index_revised.csv")
    result_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    print("\n" + "=" * 60)
    print(f"âœ… ì‘ì—… ì™„ë£Œ: {output_path}")
    print(f"   - ë°ì´í„° ê¸°ê°„: {result_df['start_date'].min().date()} ~ {result_df['end_date'].max().date()}")
    print(f"   - ì´ ë°ì´í„° í–‰: {len(result_df)}ê°œ")
    print("=" * 60)

if __name__ == "__main__":
    main()
